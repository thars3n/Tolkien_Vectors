{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and set up logging\n",
    "import spacy\n",
    "import gensim \n",
    "import logging\n",
    "import glob, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing all source texts for training the model \n",
    "data_dir=\"../corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hobbit_input_utf8.txt = 508213 chars\n",
      "LostTales1_input_utf8.txt = 383135 chars\n",
      "LostTales2_input_utf8.txt = 462819 chars\n",
      "LotR_complete_input_utf8.txt = 2565751 chars\n",
      "Silmarillion_input_utf8.txt = 698110 chars\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_dir)\n",
    "documents = []\n",
    "for filename in sorted(glob.glob(\"*.txt\")):\n",
    "    filedata = open(filename, 'r').read()\n",
    "    print(filename + \" = \" + str(len(filedata)) + \" chars\")\n",
    "    documents.append(filedata)\n",
    "    #documents = documents + filedata.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 39s, sys: 39.8 s, total: 14min 19s\n",
      "Wall time: 14min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Use the NLP library SpaCy (spacy.io) to cut into sentences, and remove all punctuation and extra spaces (etc.)\n",
    "#\n",
    "# We could also lowercase and lemmatize everything here, and remove stopwords, but we aren't going to for now.\n",
    "#\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "all_sentences = []\n",
    "for doc in documents:\n",
    "    # replace all the line feeds with spaces\n",
    "    doc = doc.replace(\"\\n\", \" \")\n",
    "    # run the spacy tokenization/nlp algorithm on each source document\n",
    "    spacy_doc = nlp(doc)\n",
    "    for spacy_sentence in spacy_doc.sents:\n",
    "        #print(spacy_sentence)\n",
    "        sentence_clean = []\n",
    "        for token in spacy_sentence:\n",
    "            if token.pos_ != \"SPACE\" and token.pos_ != \"PUNCT\":\n",
    "                sentence_clean.append(token.text)\n",
    "        cleaned_sentence = \" \".join(sentence_clean)\n",
    "        all_sentences.append(cleaned_sentence)\n",
    "        #print(cleaned_sentence)\n",
    "        #print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54766\n"
     ]
    }
   ],
   "source": [
    "# Check our total number of sentences in the corpus\n",
    "print(str(len(all_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 09:57:21,739 : INFO : collecting all words and their counts\n",
      "2021-10-09 09:57:21,747 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2021-10-09 09:57:22,099 : INFO : PROGRESS: at sentence #10000, processed 201113 words and 98653 word types\n",
      "2021-10-09 09:57:22,386 : INFO : PROGRESS: at sentence #20000, processed 365659 words and 153215 word types\n",
      "2021-10-09 09:57:22,627 : INFO : PROGRESS: at sentence #30000, processed 491531 words and 186645 word types\n",
      "2021-10-09 09:57:22,843 : INFO : PROGRESS: at sentence #40000, processed 610994 words and 216901 word types\n",
      "2021-10-09 09:57:23,096 : INFO : PROGRESS: at sentence #50000, processed 752697 words and 248421 word types\n",
      "2021-10-09 09:57:23,317 : INFO : collected 274160 word types from a corpus of 876493 words (unigram + bigrams) and 54766 sentences\n",
      "2021-10-09 09:57:23,318 : INFO : using 274160 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2021-10-09 09:57:23,319 : INFO : source_vocab length 274160\n",
      "2021-10-09 09:57:25,965 : INFO : Phraser built with 1348 phrasegrams\n",
      "2021-10-09 09:57:25,973 : INFO : collecting all words and their counts\n",
      "2021-10-09 09:57:25,974 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2021-10-09 09:57:26,889 : INFO : PROGRESS: at sentence #10000, processed 193040 words and 102251 word types\n",
      "2021-10-09 09:57:27,639 : INFO : PROGRESS: at sentence #20000, processed 349628 words and 160476 word types\n",
      "2021-10-09 09:57:28,231 : INFO : PROGRESS: at sentence #30000, processed 468246 words and 196904 word types\n",
      "2021-10-09 09:57:28,790 : INFO : PROGRESS: at sentence #40000, processed 581104 words and 229773 word types\n",
      "2021-10-09 09:57:29,450 : INFO : PROGRESS: at sentence #50000, processed 715348 words and 264274 word types\n",
      "2021-10-09 09:57:30,008 : INFO : collected 291755 word types from a corpus of 834521 words (unigram + bigrams) and 54766 sentences\n",
      "2021-10-09 09:57:30,009 : INFO : using 291755 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2021-10-09 09:57:30,009 : INFO : source_vocab length 291755\n",
      "2021-10-09 09:57:33,091 : INFO : Phraser built with 2912 phrasegrams\n",
      "2021-10-09 09:57:40,748 : INFO : collecting all words and their counts\n",
      "2021-10-09 09:57:40,749 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-10-09 09:57:40,795 : INFO : PROGRESS: at sentence #10000, processed 190412 words, keeping 12660 word types\n",
      "2021-10-09 09:57:40,835 : INFO : PROGRESS: at sentence #20000, processed 344102 words, keeping 16704 word types\n",
      "2021-10-09 09:57:40,867 : INFO : PROGRESS: at sentence #30000, processed 459940 words, keeping 18567 word types\n",
      "2021-10-09 09:57:40,897 : INFO : PROGRESS: at sentence #40000, processed 570085 words, keeping 20111 word types\n",
      "2021-10-09 09:57:40,932 : INFO : PROGRESS: at sentence #50000, processed 701631 words, keeping 21529 word types\n",
      "2021-10-09 09:57:40,960 : INFO : collected 22646 word types from a corpus of 819599 raw words and 54766 sentences\n",
      "2021-10-09 09:57:40,964 : INFO : Loading a fresh vocabulary\n",
      "2021-10-09 09:57:41,009 : INFO : effective_min_count=1 retains 22646 unique words (100% of original 22646, drops 0)\n",
      "2021-10-09 09:57:41,010 : INFO : effective_min_count=1 leaves 819599 word corpus (100% of original 819599, drops 0)\n",
      "2021-10-09 09:57:41,083 : INFO : deleting the raw counts dictionary of 22646 items\n",
      "2021-10-09 09:57:41,085 : INFO : sample=0.001 downsamples 41 most-common words\n",
      "2021-10-09 09:57:41,085 : INFO : downsampling leaves estimated 617011 word corpus (75.3% of prior 819599)\n",
      "2021-10-09 09:57:41,149 : INFO : estimated required memory for 22646 words and 300 dimensions: 65673400 bytes\n",
      "2021-10-09 09:57:41,150 : INFO : resetting layer weights\n",
      "2021-10-09 09:57:46,193 : INFO : training model with 20 workers on 22646 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-10-09 09:57:47,236 : INFO : EPOCH 1 - PROGRESS: at 64.91% examples, 385745 words/s, in_qsize 31, out_qsize 0\n",
      "2021-10-09 09:57:47,370 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-10-09 09:57:47,398 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-10-09 09:57:47,407 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-10-09 09:57:47,413 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-10-09 09:57:47,431 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-10-09 09:57:47,433 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-10-09 09:57:47,445 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-10-09 09:57:47,482 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-10-09 09:57:47,500 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-10-09 09:57:47,518 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-10-09 09:57:47,532 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-10-09 09:57:47,538 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-10-09 09:57:47,552 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-10-09 09:57:47,555 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-10-09 09:57:47,559 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-10-09 09:57:47,562 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-10-09 09:57:47,568 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-09 09:57:47,573 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-09 09:57:47,574 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-09 09:57:47,579 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-09 09:57:47,580 : INFO : EPOCH - 1 : training on 819599 raw words (617148 effective words) took 1.4s, 451963 effective words/s\n",
      "2021-10-09 09:57:48,612 : INFO : EPOCH 2 - PROGRESS: at 62.61% examples, 380220 words/s, in_qsize 32, out_qsize 0\n",
      "2021-10-09 09:57:48,858 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-10-09 09:57:48,865 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-10-09 09:57:48,872 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-10-09 09:57:48,874 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-10-09 09:57:48,885 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-10-09 09:57:48,898 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-10-09 09:57:48,907 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-10-09 09:57:48,931 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-10-09 09:57:48,944 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-10-09 09:57:48,952 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-10-09 09:57:48,962 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-10-09 09:57:48,968 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-10-09 09:57:48,973 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-10-09 09:57:48,978 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-10-09 09:57:48,981 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-10-09 09:57:48,982 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-10-09 09:57:48,986 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-09 09:57:48,987 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-09 09:57:48,995 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-09 09:57:49,002 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-09 09:57:49,002 : INFO : EPOCH - 2 : training on 819599 raw words (616804 effective words) took 1.4s, 438904 effective words/s\n",
      "2021-10-09 09:57:50,026 : INFO : EPOCH 3 - PROGRESS: at 53.06% examples, 339047 words/s, in_qsize 35, out_qsize 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 09:57:50,285 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-10-09 09:57:50,290 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-10-09 09:57:50,299 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-10-09 09:57:50,341 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-10-09 09:57:50,343 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-10-09 09:57:50,346 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-10-09 09:57:50,360 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-10-09 09:57:50,365 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-10-09 09:57:50,397 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-10-09 09:57:50,410 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-10-09 09:57:50,449 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-10-09 09:57:50,468 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-10-09 09:57:50,494 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-10-09 09:57:50,499 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-10-09 09:57:50,502 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-10-09 09:57:50,503 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-10-09 09:57:50,506 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-09 09:57:50,518 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-09 09:57:50,527 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-09 09:57:50,532 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-09 09:57:50,532 : INFO : EPOCH - 3 : training on 819599 raw words (616996 effective words) took 1.5s, 409473 effective words/s\n",
      "2021-10-09 09:57:51,659 : INFO : EPOCH 4 - PROGRESS: at 48.01% examples, 284658 words/s, in_qsize 38, out_qsize 1\n",
      "2021-10-09 09:57:51,972 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-10-09 09:57:51,987 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-10-09 09:57:51,993 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-10-09 09:57:51,994 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-10-09 09:57:52,004 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-10-09 09:57:52,012 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-10-09 09:57:52,020 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-10-09 09:57:52,024 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-10-09 09:57:52,035 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-10-09 09:57:52,055 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-10-09 09:57:52,069 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-10-09 09:57:52,073 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-10-09 09:57:52,076 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-10-09 09:57:52,082 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-10-09 09:57:52,084 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-10-09 09:57:52,087 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-10-09 09:57:52,091 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-09 09:57:52,094 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-09 09:57:52,094 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-09 09:57:52,121 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-09 09:57:52,121 : INFO : EPOCH - 4 : training on 819599 raw words (616938 effective words) took 1.6s, 392456 effective words/s\n",
      "2021-10-09 09:57:53,171 : INFO : EPOCH 5 - PROGRESS: at 53.15% examples, 327939 words/s, in_qsize 38, out_qsize 0\n",
      "2021-10-09 09:57:53,443 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-10-09 09:57:53,459 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-10-09 09:57:53,500 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-10-09 09:57:53,502 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-10-09 09:57:53,515 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-10-09 09:57:53,523 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-10-09 09:57:53,527 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-10-09 09:57:53,536 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-10-09 09:57:53,587 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-10-09 09:57:53,602 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-10-09 09:57:53,622 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-10-09 09:57:53,626 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-10-09 09:57:53,630 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-10-09 09:57:53,632 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-10-09 09:57:53,633 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-10-09 09:57:53,634 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-10-09 09:57:53,638 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-09 09:57:53,649 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-09 09:57:53,651 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-09 09:57:53,658 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-09 09:57:53,658 : INFO : EPOCH - 5 : training on 819599 raw words (616958 effective words) took 1.5s, 405127 effective words/s\n",
      "2021-10-09 09:57:53,659 : INFO : training on a 4097995 raw words (3084844 effective words) took 7.5s, 413229 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JRR', 'TOLKIEN', 'THE', 'HOBBIT', 'Chapter', 'I', 'An', 'Unexpected', 'Party', 'In']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "#documents = [\"the mayor of new york was there\", \"human computer interaction and machine learning has now become a trending research area\",\"human computer interaction is interesting\",\"human computer interaction is a pretty interesting subject\", \"human computer interaction is a great and new subject\", \"machine learning can be useful sometimes\",\"new york mayor was present\", \"I love machine learning because it is a new subject area\", \"human computer interaction helps people to get user friendly applications\"]\n",
    "\n",
    "sentence_stream = [doc.split(\" \") for doc in all_sentences]  #documents\n",
    "\n",
    "trigram_sentences_project = []\n",
    "\n",
    "bigram = Phraser(Phrases(sentence_stream))\n",
    "trigram = Phraser(Phrases(bigram[sentence_stream]))\n",
    "\n",
    "for sent in sentence_stream:\n",
    "    bigrams_ = bigram[sent]\n",
    "    trigrams_ = trigram[bigram[sent]]\n",
    "    trigram_sentences_project.append(trigrams_)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 1    # Minimum word count                        \n",
    "num_workers = 20      # Number of threads to run in parallel\n",
    "context = 5           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "skip_grams = 1        # 0 for CBOW, 1 for skip-grams\n",
    "\n",
    "model = word2vec.Word2Vec(trigram_sentences_project, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, sg = skip_grams)\n",
    "\n",
    "vocab = list(model.wv.vocab.keys())\n",
    "print(vocab[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22646\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of items in our model's vocabulary\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-09 09:57:53,692 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Gollum', 0.9023342132568359),\n",
       " ('Sam', 0.8929318785667419),\n",
       " ('Frodo', 0.8768751621246338),\n",
       " ('Pippin', 0.8751285076141357),\n",
       " ('Strider', 0.8512952327728271),\n",
       " ('Shagrat', 0.8474864959716797),\n",
       " ('Merry', 0.8417835235595703),\n",
       " ('wizard', 0.8417626619338989),\n",
       " ('Gandalf', 0.835616946220398),\n",
       " ('Tom', 0.8336421847343445)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"Bilbo\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rider', 0.8844809532165527),\n",
       " ('Rider', 0.8839728832244873),\n",
       " ('orc', 0.8817406892776489),\n",
       " ('enemy', 0.8769322633743286),\n",
       " ('sign', 0.8714866638183594),\n",
       " ('key', 0.8694632053375244),\n",
       " ('partly', 0.8665182590484619),\n",
       " ('challenge', 0.8615357875823975),\n",
       " ('Great_Goblin', 0.8601824641227722),\n",
       " ('accident', 0.8601601719856262)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"dragon\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('holes', 0.8676007390022278),\n",
       " ('shapes', 0.8624780774116516),\n",
       " ('lights', 0.8590327501296997),\n",
       " ('bushes', 0.8565700054168701),\n",
       " ('thickets', 0.8529249429702759),\n",
       " ('reeds', 0.8488107919692993),\n",
       " ('grasses', 0.8482720851898193),\n",
       " ('noises', 0.8469080924987793),\n",
       " ('lines', 0.844353973865509),\n",
       " ('points', 0.8439342379570007)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"trees\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tharsen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# write the input files to display via https://projector.tensorflow.org/\n",
    "tensorsfp = \"../vectors.txt\"\n",
    "metadatafp = \"../metadata.txt\"\n",
    "\n",
    "with open( tensorsfp, 'w+') as tensors:\n",
    "    with open( metadatafp, 'w+') as metadata:\n",
    "         for word in model.wv.index2word:\n",
    "                metadata.write(word + '\\n')\n",
    "                vector_row = '\\t'.join(map(str, model[word]))\n",
    "                tensors.write(vector_row + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
