{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and set up logging\n",
    "import spacy\n",
    "import gensim \n",
    "import logging\n",
    "import glob, os\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing all source texts for training the model \n",
    "data_dir=\"../corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hobbit_input_utf8.txt = 508213 chars\n",
      "LostTales1_input_utf8.txt = 383135 chars\n",
      "LostTales2_input_utf8.txt = 462819 chars\n",
      "LotR_complete_input_utf8.txt = 2565751 chars\n",
      "Silmarillion_input_utf8.txt = 698110 chars\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_dir)\n",
    "documents = []\n",
    "for filename in sorted(glob.glob(\"*.txt\")):\n",
    "    filedata = open(filename, 'r').read()\n",
    "    print(filename + \" = \" + str(len(filedata)) + \" chars\")\n",
    "    documents.append(filedata)\n",
    "    #documents = documents + filedata.split(\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 56s, sys: 42.1 s, total: 14min 38s\n",
      "Wall time: 14min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Use the NLP library SpaCy (spacy.io) to cut into sentences, and remove all punctuation and extra spaces (etc.)\n",
    "#\n",
    "# We could also lowercase and lemmatize everything here, and remove stopwords, but we aren't going to for now.\n",
    "#\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "all_sentences = []\n",
    "for doc in documents:\n",
    "    # replace all the line feeds with spaces\n",
    "    doc = doc.replace(\"\\n\", \" \")\n",
    "    # run the spacy tokenization/nlp algorithm on each source document\n",
    "    spacy_doc = nlp(doc)\n",
    "    for spacy_sentence in spacy_doc.sents:\n",
    "        #print(spacy_sentence)\n",
    "        sentence_clean = []\n",
    "        for token in spacy_sentence:\n",
    "            if token.pos_ != \"SPACE\" and token.pos_ != \"PUNCT\":\n",
    "                sentence_clean.append(token.text)\n",
    "        cleaned_sentence = \" \".join(sentence_clean)\n",
    "        all_sentences.append(cleaned_sentence)\n",
    "        #print(cleaned_sentence)\n",
    "        #print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "54765\n"
     ]
    }
   ],
   "source": [
    "# Check our total number of sentences in the corpus\n",
    "print(str(len(all_sentences)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-07 16:56:57,240 : INFO : collecting all words and their counts\n",
      "2021-10-07 16:56:57,241 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2021-10-07 16:56:57,568 : INFO : PROGRESS: at sentence #10000, processed 201113 words and 98653 word types\n",
      "2021-10-07 16:56:57,832 : INFO : PROGRESS: at sentence #20000, processed 365659 words and 153214 word types\n",
      "2021-10-07 16:56:58,049 : INFO : PROGRESS: at sentence #30000, processed 491546 words and 186647 word types\n",
      "2021-10-07 16:56:58,253 : INFO : PROGRESS: at sentence #40000, processed 611015 words and 216902 word types\n",
      "2021-10-07 16:56:58,486 : INFO : PROGRESS: at sentence #50000, processed 752714 words and 248416 word types\n",
      "2021-10-07 16:56:58,694 : INFO : collected 274159 word types from a corpus of 876492 words (unigram + bigrams) and 54765 sentences\n",
      "2021-10-07 16:56:58,695 : INFO : using 274159 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2021-10-07 16:56:58,696 : INFO : source_vocab length 274159\n",
      "2021-10-07 16:57:01,319 : INFO : Phraser built with 1349 phrasegrams\n",
      "2021-10-07 16:57:01,325 : INFO : collecting all words and their counts\n",
      "2021-10-07 16:57:01,327 : INFO : PROGRESS: at sentence #0, processed 0 words and 0 word types\n",
      "2021-10-07 16:57:02,208 : INFO : PROGRESS: at sentence #10000, processed 193040 words and 102251 word types\n",
      "2021-10-07 16:57:02,933 : INFO : PROGRESS: at sentence #20000, processed 349626 words and 160476 word types\n",
      "2021-10-07 16:57:03,508 : INFO : PROGRESS: at sentence #30000, processed 468255 words and 196914 word types\n",
      "2021-10-07 16:57:04,142 : INFO : PROGRESS: at sentence #40000, processed 581115 words and 229784 word types\n",
      "2021-10-07 16:57:04,895 : INFO : PROGRESS: at sentence #50000, processed 715354 words and 264279 word types\n",
      "2021-10-07 16:57:05,435 : INFO : collected 291764 word types from a corpus of 834510 words (unigram + bigrams) and 54765 sentences\n",
      "2021-10-07 16:57:05,436 : INFO : using 291764 counts as vocab in Phrases<0 vocab, min_count=5, threshold=10.0, max_vocab_size=40000000>\n",
      "2021-10-07 16:57:05,437 : INFO : source_vocab length 291764\n",
      "2021-10-07 16:57:08,563 : INFO : Phraser built with 2913 phrasegrams\n",
      "2021-10-07 16:57:16,101 : INFO : collecting all words and their counts\n",
      "2021-10-07 16:57:16,101 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2021-10-07 16:57:16,142 : INFO : PROGRESS: at sentence #10000, processed 190411 words, keeping 12661 word types\n",
      "2021-10-07 16:57:16,176 : INFO : PROGRESS: at sentence #20000, processed 344098 words, keeping 16704 word types\n",
      "2021-10-07 16:57:16,207 : INFO : PROGRESS: at sentence #30000, processed 459950 words, keeping 18568 word types\n",
      "2021-10-07 16:57:16,236 : INFO : PROGRESS: at sentence #40000, processed 570098 words, keeping 20113 word types\n",
      "2021-10-07 16:57:16,271 : INFO : PROGRESS: at sentence #50000, processed 701640 words, keeping 21530 word types\n",
      "2021-10-07 16:57:16,300 : INFO : collected 22647 word types from a corpus of 819590 raw words and 54765 sentences\n",
      "2021-10-07 16:57:16,301 : INFO : Loading a fresh vocabulary\n",
      "2021-10-07 16:57:16,343 : INFO : effective_min_count=1 retains 22647 unique words (100% of original 22647, drops 0)\n",
      "2021-10-07 16:57:16,344 : INFO : effective_min_count=1 leaves 819590 word corpus (100% of original 819590, drops 0)\n",
      "2021-10-07 16:57:16,415 : INFO : deleting the raw counts dictionary of 22647 items\n",
      "2021-10-07 16:57:16,416 : INFO : sample=0.001 downsamples 41 most-common words\n",
      "2021-10-07 16:57:16,417 : INFO : downsampling leaves estimated 617001 word corpus (75.3% of prior 819590)\n",
      "2021-10-07 16:57:16,489 : INFO : estimated required memory for 22647 words and 300 dimensions: 65676300 bytes\n",
      "2021-10-07 16:57:16,490 : INFO : resetting layer weights\n",
      "2021-10-07 16:57:21,648 : INFO : training model with 20 workers on 22647 vocabulary and 300 features, using sg=1 hs=0 sample=0.001 negative=5 window=5\n",
      "2021-10-07 16:57:22,826 : INFO : EPOCH 1 - PROGRESS: at 47.92% examples, 272673 words/s, in_qsize 38, out_qsize 1\n",
      "2021-10-07 16:57:23,161 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-10-07 16:57:23,167 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-10-07 16:57:23,168 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-10-07 16:57:23,170 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-10-07 16:57:23,191 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-10-07 16:57:23,197 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-10-07 16:57:23,201 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-10-07 16:57:23,230 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-10-07 16:57:23,241 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-10-07 16:57:23,252 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-10-07 16:57:23,254 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-10-07 16:57:23,261 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-10-07 16:57:23,263 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-10-07 16:57:23,265 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-10-07 16:57:23,267 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-10-07 16:57:23,268 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-10-07 16:57:23,269 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-07 16:57:23,272 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-07 16:57:23,273 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-07 16:57:23,301 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-07 16:57:23,304 : INFO : EPOCH - 1 : training on 819590 raw words (616756 effective words) took 1.6s, 377068 effective words/s\n",
      "2021-10-07 16:57:24,352 : INFO : EPOCH 2 - PROGRESS: at 52.76% examples, 327911 words/s, in_qsize 38, out_qsize 0\n",
      "2021-10-07 16:57:24,628 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-10-07 16:57:24,645 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-10-07 16:57:24,653 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-10-07 16:57:24,670 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-10-07 16:57:24,675 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-10-07 16:57:24,686 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-10-07 16:57:24,700 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-10-07 16:57:24,705 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-10-07 16:57:24,761 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-10-07 16:57:24,767 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-10-07 16:57:24,776 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-10-07 16:57:24,790 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-10-07 16:57:24,796 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-10-07 16:57:24,797 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-10-07 16:57:24,806 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-10-07 16:57:24,807 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-10-07 16:57:24,809 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-07 16:57:24,812 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-07 16:57:24,813 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-07 16:57:24,814 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-07 16:57:24,815 : INFO : EPOCH - 2 : training on 819590 raw words (616747 effective words) took 1.5s, 411946 effective words/s\n",
      "2021-10-07 16:57:25,911 : INFO : EPOCH 3 - PROGRESS: at 51.51% examples, 307470 words/s, in_qsize 39, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-07 16:57:26,152 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-10-07 16:57:26,235 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-10-07 16:57:26,242 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-10-07 16:57:26,273 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-10-07 16:57:26,274 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-10-07 16:57:26,285 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-10-07 16:57:26,293 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-10-07 16:57:26,312 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-10-07 16:57:26,319 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-10-07 16:57:26,342 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-10-07 16:57:26,344 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-10-07 16:57:26,352 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-10-07 16:57:26,357 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-10-07 16:57:26,361 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-10-07 16:57:26,364 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-10-07 16:57:26,369 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-10-07 16:57:26,371 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-07 16:57:26,373 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-07 16:57:26,375 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-07 16:57:26,380 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-07 16:57:26,380 : INFO : EPOCH - 3 : training on 819590 raw words (616880 effective words) took 1.5s, 398280 effective words/s\n",
      "2021-10-07 16:57:27,477 : INFO : EPOCH 4 - PROGRESS: at 49.59% examples, 300363 words/s, in_qsize 39, out_qsize 0\n",
      "2021-10-07 16:57:27,815 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-10-07 16:57:27,824 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-10-07 16:57:27,832 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-10-07 16:57:27,840 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-10-07 16:57:27,842 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-10-07 16:57:27,848 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-10-07 16:57:27,857 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-10-07 16:57:27,864 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-10-07 16:57:27,874 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-10-07 16:57:27,880 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-10-07 16:57:27,895 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-10-07 16:57:27,911 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-10-07 16:57:27,941 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-10-07 16:57:27,955 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-10-07 16:57:27,958 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-10-07 16:57:27,960 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-10-07 16:57:27,962 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-07 16:57:27,964 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-07 16:57:27,965 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-07 16:57:27,967 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-07 16:57:27,968 : INFO : EPOCH - 4 : training on 819590 raw words (617554 effective words) took 1.6s, 393287 effective words/s\n",
      "2021-10-07 16:57:28,996 : INFO : EPOCH 5 - PROGRESS: at 26.03% examples, 198849 words/s, in_qsize 33, out_qsize 12\n",
      "2021-10-07 16:57:29,522 : INFO : worker thread finished; awaiting finish of 19 more threads\n",
      "2021-10-07 16:57:29,533 : INFO : worker thread finished; awaiting finish of 18 more threads\n",
      "2021-10-07 16:57:29,559 : INFO : worker thread finished; awaiting finish of 17 more threads\n",
      "2021-10-07 16:57:29,565 : INFO : worker thread finished; awaiting finish of 16 more threads\n",
      "2021-10-07 16:57:29,584 : INFO : worker thread finished; awaiting finish of 15 more threads\n",
      "2021-10-07 16:57:29,639 : INFO : worker thread finished; awaiting finish of 14 more threads\n",
      "2021-10-07 16:57:29,646 : INFO : worker thread finished; awaiting finish of 13 more threads\n",
      "2021-10-07 16:57:29,651 : INFO : worker thread finished; awaiting finish of 12 more threads\n",
      "2021-10-07 16:57:29,665 : INFO : worker thread finished; awaiting finish of 11 more threads\n",
      "2021-10-07 16:57:29,667 : INFO : worker thread finished; awaiting finish of 10 more threads\n",
      "2021-10-07 16:57:29,684 : INFO : worker thread finished; awaiting finish of 9 more threads\n",
      "2021-10-07 16:57:29,734 : INFO : worker thread finished; awaiting finish of 8 more threads\n",
      "2021-10-07 16:57:29,745 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2021-10-07 16:57:29,748 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2021-10-07 16:57:29,750 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2021-10-07 16:57:29,753 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2021-10-07 16:57:29,755 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2021-10-07 16:57:29,758 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2021-10-07 16:57:29,765 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2021-10-07 16:57:29,766 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2021-10-07 16:57:29,767 : INFO : EPOCH - 5 : training on 819590 raw words (616783 effective words) took 1.8s, 346047 effective words/s\n",
      "2021-10-07 16:57:29,768 : INFO : training on a 4097950 raw words (3084720 effective words) took 8.1s, 379948 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['JRR', 'TOLKIEN', 'THE', 'HOBBIT', 'Chapter', 'I', 'An', 'Unexpected', 'Party', 'In']\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import word2vec, Phrases\n",
    "from gensim.models.phrases import Phraser\n",
    "\n",
    "#documents = [\"the mayor of new york was there\", \"human computer interaction and machine learning has now become a trending research area\",\"human computer interaction is interesting\",\"human computer interaction is a pretty interesting subject\", \"human computer interaction is a great and new subject\", \"machine learning can be useful sometimes\",\"new york mayor was present\", \"I love machine learning because it is a new subject area\", \"human computer interaction helps people to get user friendly applications\"]\n",
    "\n",
    "sentence_stream = [doc.split(\" \") for doc in all_sentences]  #documents\n",
    "\n",
    "trigram_sentences_project = []\n",
    "\n",
    "bigram = Phraser(Phrases(sentence_stream))\n",
    "trigram = Phraser(Phrases(bigram[sentence_stream]))\n",
    "\n",
    "for sent in sentence_stream:\n",
    "    bigrams_ = bigram[sent]\n",
    "    trigrams_ = trigram[bigram[sent]]\n",
    "    trigram_sentences_project.append(trigrams_)\n",
    "\n",
    "# Set values for various parameters\n",
    "num_features = 300    # Word vector dimensionality                      \n",
    "min_word_count = 1    # Minimum word count                        \n",
    "num_workers = 20      # Number of threads to run in parallel\n",
    "context = 5           # Context window size                                                                                    \n",
    "downsampling = 1e-3   # Downsample setting for frequent words\n",
    "skip_grams = 1        # 0 for CBOW, 1 for skip-grams\n",
    "\n",
    "model = word2vec.Word2Vec(trigram_sentences_project, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, sg = skip_grams)\n",
    "\n",
    "vocab = list(model.wv.vocab.keys())\n",
    "print(vocab[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22647\n"
     ]
    }
   ],
   "source": [
    "# Print the total number of items in our model's vocabulary\n",
    "print(len(model.wv.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-07 16:57:29,799 : INFO : precomputing L2-norms of word weight vectors\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('Sam', 0.8769451975822449),\n",
       " ('Gollum', 0.8649412393569946),\n",
       " ('Pippin', 0.8555766344070435),\n",
       " ('Frodo', 0.849555492401123),\n",
       " ('Strider', 0.8470858335494995),\n",
       " ('Tom', 0.8436428308486938),\n",
       " ('Mr._Butterbur', 0.8343122005462646),\n",
       " ('Thorin', 0.8338472843170166),\n",
       " ('Shagrat', 0.8319171667098999),\n",
       " ('Merry', 0.8291702270507812)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"Bilbo\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('rider', 0.8914847373962402),\n",
       " ('sign', 0.8842803835868835),\n",
       " ('enemy', 0.8774009943008423),\n",
       " ('key', 0.8722566962242126),\n",
       " ('wolf', 0.8716298937797546),\n",
       " ('troll', 0.8692148327827454),\n",
       " ('Nazgyl', 0.8679828643798828),\n",
       " ('Great_Goblin', 0.8665998578071594),\n",
       " ('Rider', 0.8664273619651794),\n",
       " ('marked', 0.8639983534812927)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"dragon\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('shapes', 0.8648653030395508),\n",
       " ('bushes', 0.8618569374084473),\n",
       " ('rocks', 0.8409314751625061),\n",
       " ('holes', 0.837933361530304),\n",
       " ('thickets', 0.836373507976532),\n",
       " ('stones', 0.8329076170921326),\n",
       " ('points', 0.8316853642463684),\n",
       " ('swans', 0.830091118812561),\n",
       " ('grass', 0.8296234011650085),\n",
       " ('lines', 0.8273912668228149)]"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w1 = \"trees\"\n",
    "model.wv.most_similar (positive=w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tharsen/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:9: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "# write the input files to display via https://projector.tensorflow.org/\n",
    "tensorsfp = \"../vectors.txt\"\n",
    "metadatafp = \"../metadata.txt\"\n",
    "\n",
    "with open( tensorsfp, 'w+') as tensors:\n",
    "    with open( metadatafp, 'w+') as metadata:\n",
    "         for word in model.wv.index2word:\n",
    "                metadata.write(word + '\\n')\n",
    "                vector_row = '\\t'.join(map(str, model[word]))\n",
    "                tensors.write(vector_row + '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
