{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "handed-valuation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports and set up logging\n",
    "import spacy\n",
    "import glob, os\n",
    "from collections import Counter\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "abandoned-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# directory containing all source texts for training the model \n",
    "data_dir=\"../corpus\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "boxed-recruitment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LotR_complete_input_utf8.txt = 2565751 chars\n"
     ]
    }
   ],
   "source": [
    "os.chdir(data_dir)\n",
    "\n",
    "documents = []\n",
    "filenames = []\n",
    "\n",
    "for filename in sorted(glob.glob(\"*.txt\")):\n",
    "    filedata = open(filename, 'r').read()\n",
    "    print(filename + \" = \" + str(len(filedata)) + \" chars\")\n",
    "    documents.append(filedata)\n",
    "    filenames.append(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "durable-ebony",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 19s, sys: 29.1 s, total: 11min 48s\n",
      "Wall time: 11min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Use the NLP library SpaCy (spacy.io) to find all tokens (words) and lemmas (word roots)\n",
    "#  that are not spaces, punctuation or \"stopwords\" (common words)\n",
    "\n",
    "# Load the model\n",
    "nlp = spacy.load('en_core_web_lg')\n",
    "nlp.max_length = 3000000\n",
    "\n",
    "doc_words = []\n",
    "doc_lemmas = []\n",
    "for doc in documents:\n",
    "    all_words = []\n",
    "    all_lemmas = []\n",
    "    # replace all the line feeds with spaces\n",
    "    doc = doc.replace(\"\\n\", \" \")\n",
    "    # run the spacy tokenization/nlp algorithm on each source document\n",
    "    spacy_doc = nlp(doc)\n",
    "    for token in spacy_doc:\n",
    "        if token.pos_ != \"SPACE\" and token.pos_ != \"PUNCT\" and token.is_stop != True:\n",
    "            all_words.append(token.text)\n",
    "            all_lemmas.append(token.lemma_)\n",
    "    doc_words.append(all_words)\n",
    "    doc_lemmas.append(all_lemmas)\n",
    "\n",
    "#print(all_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ranging-bread",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common words in LotR_complete_input_utf8.txt\n",
      "\n",
      "[('said', 4107),\n",
      " ('Frodo', 1988),\n",
      " ('Sam', 1290),\n",
      " ('came', 1252),\n",
      " ('Gandalf', 1120),\n",
      " ('like', 1104),\n",
      " ('long', 1101),\n",
      " ('great', 1033),\n",
      " ('come', 979),\n",
      " ('away', 923),\n",
      " ('far', 762),\n",
      " ('way', 735),\n",
      " ('Aragorn', 722),\n",
      " ('went', 707),\n",
      " ('Pippin', 685),\n",
      " ('know', 672),\n",
      " ('time', 666),\n",
      " ('dark', 664),\n",
      " ('shall', 662),\n",
      " ('old', 635),\n",
      " ('looked', 619),\n",
      " ('eyes', 613),\n",
      " (\"'\", 578),\n",
      " ('light', 578),\n",
      " ('little', 572),\n",
      " ('Merry', 544),\n",
      " ('thought', 539),\n",
      " ('saw', 538),\n",
      " ('stood', 511),\n",
      " ('night', 502),\n",
      " ('hand', 499),\n",
      " ('heard', 488),\n",
      " ('think', 481),\n",
      " ('hobbits', 481),\n",
      " ('passed', 472),\n",
      " ('left', 465),\n",
      " ('day', 462),\n",
      " ('men', 459),\n",
      " ('road', 438),\n",
      " ('good', 425),\n",
      " ('trees', 414),\n",
      " ('things', 412),\n",
      " ('Gollum', 406),\n",
      " ('going', 406),\n",
      " ('land', 396),\n",
      " ('found', 392),\n",
      " ('turned', 390),\n",
      " ('Gimli', 390),\n",
      " ('fell', 387),\n",
      " ('end', 378),\n",
      " ('seen', 375),\n",
      " ('water', 371),\n",
      " ('hope', 370),\n",
      " ('cried', 369),\n",
      " ('lay', 368),\n",
      " ('Bilbo', 367),\n",
      " ('black', 361),\n",
      " ('find', 354),\n",
      " ('Ring', 352),\n",
      " ('feet', 346),\n",
      " ('set', 345),\n",
      " ('Legolas', 335),\n",
      " ('right', 334),\n",
      " ('days', 331),\n",
      " ('deep', 331),\n",
      " ('head', 328),\n",
      " ('voice', 328),\n",
      " ('grey', 326),\n",
      " ('high', 325),\n",
      " ('Shire', 322),\n",
      " ('white', 322),\n",
      " ('soon', 318),\n",
      " ('Mr.', 317),\n",
      " ('fear', 315),\n",
      " ('Men', 312),\n",
      " ('suddenly', 311),\n",
      " ('stone', 310),\n",
      " ('got', 307),\n",
      " ('round', 306),\n",
      " ('folk', 304),\n",
      " ('face', 300),\n",
      " ('man', 297),\n",
      " ('look', 297),\n",
      " ('felt', 293),\n",
      " ('took', 292),\n",
      " ('Faramir', 292),\n",
      " ('gone', 290),\n",
      " ('fire', 290),\n",
      " ('heart', 289),\n",
      " ('began', 288),\n",
      " ('wind', 288),\n",
      " ('sat', 285),\n",
      " ('answered', 284),\n",
      " ('words', 281),\n",
      " ('moment', 281),\n",
      " ('Saruman', 281),\n",
      " ('Boromir', 278),\n",
      " ('Ores', 276),\n",
      " ('need', 276),\n",
      " ('tell', 276)]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Provide the top 100 most frequent words for each source\n",
    "\n",
    "filecounter = 0\n",
    "for word_doc in doc_words:\n",
    "    most_freq_words = Counter(word_doc)\n",
    "    common_words = most_freq_words.most_common(100)\n",
    "    print(\"Most common words in \" + filenames[filecounter] + \"\\n\")\n",
    "    pprint(common_words)\n",
    "    print(\"-----\")\n",
    "    # Let's also write each list to a file\n",
    "    outfilename = \"../\" + filenames[filecounter] + \"_most_common_words.txt\"\n",
    "    outfile = open(outfilename, 'w')\n",
    "    outfile.write(\"Most common words in \" + filenames[filecounter] + \"\\n\\n\")\n",
    "    for word, count in common_words:\n",
    "        outfile.write(word + \"\\t\" + str(count) + \"\\n\")\n",
    "    outfile.close()\n",
    "    # iterate\n",
    "    filecounter += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "subsequent-modeling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most common lemmas in LotR_complete_input_utf8.txt\n",
      "[('say', 4223),\n",
      " ('come', 2749),\n",
      " ('Frodo', 1988),\n",
      " ('go', 1482),\n",
      " ('long', 1329),\n",
      " ('Sam', 1290),\n",
      " ('look', 1233),\n",
      " ('great', 1156),\n",
      " ('like', 1147),\n",
      " ('know', 1139),\n",
      " ('Gandalf', 1120),\n",
      " ('see', 968),\n",
      " ('away', 949),\n",
      " ('think', 938),\n",
      " ('man', 874),\n",
      " ('day', 831),\n",
      " ('far', 812),\n",
      " ('time', 808),\n",
      " ('way', 800),\n",
      " ('fall', 787),\n",
      " ('find', 778),\n",
      " ('leave', 773),\n",
      " ('pass', 765),\n",
      " ('hand', 759),\n",
      " ('stand', 744),\n",
      " ('hear', 734),\n",
      " ('light', 731),\n",
      " ('Aragorn', 722),\n",
      " ('eye', 720),\n",
      " ('dark', 696),\n",
      " ('old', 693),\n",
      " ('Pippin', 685),\n",
      " ('hobbit', 684),\n",
      " ('shall', 682),\n",
      " ('turn', 666),\n",
      " ('thing', 663),\n",
      " ('speak', 637),\n",
      " ('lie', 629),\n",
      " ('tree', 608),\n",
      " ('good', 597),\n",
      " ('little', 582),\n",
      " (\"'\", 578),\n",
      " ('cry', 569),\n",
      " ('night', 554),\n",
      " ('Merry', 544),\n",
      " ('road', 541),\n",
      " ('land', 538),\n",
      " ('tell', 508),\n",
      " ('end', 506),\n",
      " ('ride', 501),\n",
      " ('take', 499),\n",
      " ('foot', 485),\n",
      " ('feel', 480),\n",
      " ('voice', 469),\n",
      " ('run', 466),\n",
      " ('answer', 461),\n",
      " ('let', 460),\n",
      " ('shadow', 459),\n",
      " ('stone', 439),\n",
      " ('water', 438),\n",
      " ('hope', 438),\n",
      " ('sit', 431),\n",
      " ('grow', 428),\n",
      " ('suddenly', 423),\n",
      " ('face', 421),\n",
      " ('soon', 421),\n",
      " ('fear', 418),\n",
      " ('hill', 416),\n",
      " ('word', 415),\n",
      " ('begin', 413),\n",
      " ('head', 410),\n",
      " ('Gollum', 406),\n",
      " ('deep', 404),\n",
      " ('wall', 402),\n",
      " ('Gimli', 390),\n",
      " ('get', 388),\n",
      " ('high', 377),\n",
      " ('draw', 377),\n",
      " ('wind', 377),\n",
      " ('set', 376),\n",
      " ('heart', 375),\n",
      " ('black', 374),\n",
      " ('near', 373),\n",
      " ('Bilbo', 367),\n",
      " ('king', 358),\n",
      " ('need', 355),\n",
      " ('bring', 353),\n",
      " ('yes', 351),\n",
      " ('right', 350),\n",
      " ('fire', 349),\n",
      " ('ask', 343),\n",
      " ('place', 342),\n",
      " ('Ring', 337),\n",
      " ('Legolas', 335),\n",
      " ('hold', 332),\n",
      " ('white', 332),\n",
      " ('walk', 332),\n",
      " ('rise', 332),\n",
      " ('door', 331),\n",
      " ('grey', 330)]\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "# Provide the top 100 most frequent lemmas for each source\n",
    "\n",
    "filecounter = 0\n",
    "for lemma_doc in doc_lemmas:\n",
    "    most_freq_lemmas = Counter(lemma_doc)\n",
    "    common_lemmas = most_freq_lemmas.most_common(100)\n",
    "    print(\"Most common lemmas in \" + filenames[filecounter])\n",
    "    pprint(common_lemmas)\n",
    "    print(\"-----\")\n",
    "    # Let's also write each list to a file\n",
    "    outfilename = \"../\" + filenames[filecounter] + \"_most_common_lemmas.txt\"\n",
    "    outfile = open(outfilename, 'w')\n",
    "    outfile.write(\"Most common lemmas in \" + filenames[filecounter] + \"\\n\\n\")\n",
    "    for lemma, count in common_lemmas:\n",
    "        outfile.write(lemma + \"\\t\" + str(count) + \"\\n\")\n",
    "    outfile.close()\n",
    "    # iterate\n",
    "    filecounter += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
